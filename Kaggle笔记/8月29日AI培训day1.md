# 8月29日AI培训 day 1

什么是数据挖掘

1.周杰伦是男歌手吗？

2.统计一下各部门上个月的销售业绩？（数据分析）

3.吸烟是否是肺炎发病的主要诱因？

数据分析：统计分析



### 机器学习

##### 数据

训练数据

验证数据

测试数据分类

###### 学习方式	

监督学习(有标签)，效果最好

无监督学习(无标签)

半监督学习(少量有标签，大量无)

###### 常见应用

回归:预测连续性的数值

分类：预测类别性数据，类别已知

聚类：预测类别性数据，类别未知



### 回归分析

#### 一元线性回归：

自变量和因变量

正相关，负相关，无关

$h_\theta(x)= \theta_0 + \theta_x$

##### 代价函数：

最小二乘法

真实值$y$,预测值$h_\theta(x)$，则误差平方为$(y-h_\theta(x))^2$

找到合适的参数，使得误差的平方和最小：

$j(\theta_0,\theta_1)= \frac{1}{2m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))^2$

代价函数表示模型的误差，代价越小，表示模型的效果越好。

###### 梯度下降法

更多是用在深度学习中。

开始时，随机初始化得到$\theta_0,\theta_1$

得到一个目标值：$j(\theta_0,\theta_1)$

然后改变$(\theta_0,\theta_1)$,直到选取到目标值最小。



学习率：迭代的步长

$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1)$



缺点：有可能选取到局部最小值。这种情况在实际中，很少取到。



#### 多元线性回归

当Y值得影响因素不是唯一时，采用多元线性回归模型

$h_\theta(x) = \theta_0+\theta_1x_1+...+\theta_nx_n$

代价函数：

$J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)^2$



### 特征缩放和交叉验证

##### 数据归一化

把数据的取值范围处理为0-1或者-1-1之间

- newValue = (oldValue-min)/(max-min)
- (1,3,5,7,9) 
- (1-1)/(9-1)=0 
- (3-1)/(9-1)=1/4 
- (5-1)/(9-1)=1/2 
- (7-1)/(9-1)=3/4 
- (9-1)/(9-1)=1

任意数据转化为-1-1之间： newValue = ((oldValue-min)/(max-min)-0.5)*2

##### 均值标准化

x为特征数据，u为数据的平均值，s为数据的方差

- newValue = (oldValue-u)/s 
- (1,3,5,7,9) 
- u = (1+3+5+7+9)/5=5 
- s = ((1-5)2+(3-5)2+(5-5)2+(7-5)2+(9-5)2)/5=8 
- (1-5)/8=-1/2 
- (3-5)/8=-1/4 
- (5-5)/8=0 
- (7-5)/8=1/4 
- (9-5)/8=1/2

##### 交叉验证法

把数据集分为N份，然后得出N个效果求平均。



### 分类

##### KNN最邻近规则分类（KNN）

1. 为了判断未知实例的类别，以所有已知类别的实例作为参照选择参数K
2. 计算未知实例与所有已知实例的距离
3. 选择最近K个已知实例
4. 根据少数服从多数的投票法则，让未知实例归类为K个最邻近样本中最多数的类别。

###### 欧式距离

两点之间的距离

###### K值选取

实际调参

###### 算法缺点

算法复杂度高（需要比较所有已知实例与要分类的实例）

当期样本分布不平衡时，受影响比较大。

###### 分类模型的评估指标

准确率：8亿人，19个恐怖分子。加入把19个恐怖分子都认为正确，那么精确率依然有99.999999% (TP+TN)/(TP+FN+TN+FP)

精确率：不平衡分类问题，TP/(TP+FP)

召回率：TP/(TP+FN)，召回率。

##### 决策树

比较适合分析离散数据

如果是连续数据需要先转成离散数据再做分析

###### 熵

一条信息的信息量大小和它的不确定性有直接的关系 ，要搞清楚一件非常非常不确定的事情，或者是我们 一无所知的事情，需要了解大量信息->信息量的度量 就等于不确定性的多少。

信息熵公式：

$H|x| = -\sum_xp(x)log_2p(x)$
假如有一个普通骰子A，仍出1-6的概率都是1/6
有一个骰子B，扔出6的概率是50%，扔出1-5的概率都是10%
有一个骰子C，扔出6的概率是100%。

个个骰子的熵计算：

骰子A：$-(\frac{1}{6}\times log_2{\frac{1}{6}})\times 6 =2.585$

骰子B: $-(\frac{1}{10}\times log_2{\frac{1}{10}})\times 5 -\frac{1}{2}\times log_2\frac{1}{2}=2.161$

骰子C: $-(1\times log_21) =0$

###### 算法

ID3算法

​	计算出各个特征的信息增益，根据信息增益的大小来判断节点在决策树中的位置

C4.5算法

​	ID3会优先选择因子数比较多的变量

​	信息增益的改进，增益率

CART算法

​	构建的决策树是二叉树。

​	用基尼（Gini）系数最小标准化准则来进行特征选择，生成二叉树

基尼系数计算：

### 集成学习

##### bagging

数据量越大，学习器性能越好。bootstrap aggregating ，是在原始数据集选择S次后得到S个新数据集的一种技术，是一种由放回抽样。	

将训练集分成5个训练集，然后训练的到5个模型，之后预测得到5个效果，然后进行投票。

##### 随机森林

bagging和决策树

- 1.样本的随机：从样本集中用bagging的方式，随机选
  择n个样本。
- 2.特征的随机：从所有属性d中随机选择k个属性(k<d)
  ，然后从k个属性中选择最佳分割属性作为节点建立
  CART决策树。
- 3.重复以上两个步骤m次，建立m棵CART决策树。
- 4.这m棵CART决策树形成随机森林，通过投票表决结
  果，决定数据属于哪一类。

##### boosting

AdaBoost是英文“Adaptive Boosting”（自适应增强）
的缩写，它的自适应在于：前一个基本分类器被错误分类的
样本的权值会增大，而正确分类的样本的权值会减小，并再
次用来训练下一个基本分类器。同时，在每一轮迭代中，加
入一个新的弱分类器，直到达到某个预定的足够小的错误率
或达到预先指定的最大迭代次数才确定最终的强分类器。

重点关注被错误分类的样本。对容易出错的样本多训练几次。

###### 步骤

Adaboost算法可以简述为三个步骤：

（1）首先，是初始化训练数据的权值分布D1。假设有N
个训练样本数据，则每一个训练样本最开始时，都被赋予
相同的权值：w1=1/N。

（2）然后，训练弱分类器hi。具体训练过程中是：如果
某个训练样本点，被弱分类器hi准确地分类，那么在构造
下一个训练集中，它对应的权值要减小；相反，如果某个
训练样本点被错误分类，那么它的权值就应该增大。权值
更新过的样本集被用于训练下一个分类器，整个训练过程
如此迭代地进行下去。

（3）最后，将各个训练得到的弱分类器组合成一个强分类
器。各个弱分类器的训练过程结束后，加大分类误差率小
的弱分类器的权重，使其在最终的分类函数中起着较大的
决定作用，而降低分类误差率大的弱分类器的权重，使其
在最终的分类函数中起着较小的决定作用。
换而言之，误差率低的弱分类器在最终分类器中占的权重
较大，否则较小。

##### ![](E:\Typora_Config\AI\python机器学习深度学习课程.jpg)Stacking

使用多个不同的分类器对训练集进行预测，把预测得到的结果作为一个次级分类器的输入。次级分类器的输出是整个模型的预测结果。

pip install mlxtend

决策树0，KNN1，神经网络1，逻辑回归0 ===》 0110-->次级分类器-->1

##### 总结

集成学习：多样性+准确性

### 聚类

##### k-means算法

- Clustering 中的经典算法，数据挖掘十大经典算法之一
- 算法接受参数k ；然后将事先输入的n个数据对象划分为k个聚类以便使得所
  获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似
  度较小。
- 算法思想：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过
  迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果

###### 算法

- 1.先从没有标签的元素集合A中随机取k个元素，作为k个子集各
  自的重心。
- 2.分别计算剩下的元素到k个子集重心的距离（这里的距离也可以
  使用欧氏距离），根据距离将这些元素分别划归到最近的子集。
- 3.根据聚类结果，重新计算重心（重心的计算方法是计算子集中
  所有元素各个维度的算数平均数）。
- 4.将集合A中全部元素按照新的重心然后再重新聚类。
- 5.重复第4步，直到聚类结果不再发生变化。



### 模型保存和载入

sklearn joblib

百万级数据可以这样用

上千万，上亿级别的数据，需要大数据级别的处理。

### 特征工程

3.1过滤无用特征

3.2特征抽象

​	处理多值有序特征map

​	处理多值无序特征one hot编码

3.3数据缩放 归一化

3.4PCA降维

​	降维后进行模型训练预测

### 处理样本不平衡

#### 采样

###### 下采样

所有数据存在DataFrame对象df中。数据分为两类：多数类别和少数类别，数据量相差大。数据预处理已将多数类别的Label标记为1，少数类别的Label标记为0。从多数类中随机抽取样本（抽取的样本数量与少数类别样本量一致）从而减少多数类别样本数据，使数据达到平衡的方式。

###### 上采样（过采样）

和欠采样采用同样的原理，通过抽样来增加少数样本的数目，从而达到数据平衡的目的。一种简单的方式就是通过有放回抽样，不断的从少数类别样本数据中抽取样本，然后使用抽取样本+原始数据组成训练数据集来训练模型；不过该方式比较容易导致过拟合，一般抽样样本不要超过50%。

因为在上采样过程中，是进行是随机有放回的抽样，所以最终模型中，数据其实是相当于存在一定的重复数据，为了防止这个重复数据导致的问题，我们可以加入一定的随机性，也就是说：在抽取数据后，对数据的各个维度可以进行随机的小范围变动，eg: (1,2,3) --> (1.01, 1.99, 3)；通过该方式可以相对比较容易的降低上采样导致的过拟合问题。
	smote

##### 混淆矩阵

[[27840 1634]        正常用户，正常预测     正常用户，预测为错

[678 1976]]			异常用户，预测正常      异常用户，预测为错



对测试集进行预测，预测之后得到概率值。

# 深度学习

### 应用

图像识别，人脸识别，图像风格转换，

### 三巨头

这几年之前，一直很冷门（一小部分人）

Hinton， google

LeCun，视觉 facebook

Bengio，

Andrew Wu, 吴恩达，课程

### 感知器学习规则

##### 单层感知器

输入节点：x1,x2,x3

输出节点：y

权向量:w1,w2,w3

偏置因子：b

激活函数：sign(x) =   1 x>=0 

​									-1 x<=0

$y = f(\sum_{i=1}^{2}x_i·{w_i})$ f是sign函数

$\Delta w_i = 𝜂(t-y)x_i$  𝜂表示学习率，t表示正确的标签，

单层感知器缺点：最开始时分类不准确，会越来越正确.但是一旦正确之后就不会再叠加，往往分类线不会最好。

##### 线性感知器

将单层感知器里面的激活函数改成：y=x



##### 激活函数

Sigmoid函数: $f(x) = \frac1{1+e^{-x}}$

tanh函数：$\frac{e^x - e^{-x}}{e^x +e^{-x}}$

Softsign函数：$\frac x{1+|x|}$

ReLU函数：f(u) = max(0,u)



##### Delta学习规则

二次代价函数

$𝐸 =\frac12(𝑡 − 𝑦)^2=\frac{1}{2}[𝑡 − 𝑓(𝑊𝑋)]^2$

梯度下降法最小化E值

##### BP神经网络

$E = \frac12(t-y)^2$

$\frac{\partial E}{\part W^l}=-(X^l)^T𝛿^l$

##### 卷积神经网络

###### CNN

卷积神经网络是近年发展起来，并广泛应用于图像处理和图像，NLP等领域的一种多层神经网络。
		传统BP处理图像时的问题：
			1.权值太多，计算量太大
			2.权值太多，需要大量样本进行训练。

###### 局部感受野

1962年哈佛医学院神经生理学家Hubel和Wiesel通过对猫视
觉皮层细胞的研究，提出了感受野(receptive field)的概念，
1984年日本学者Fukushima基于感受野概念提出的神经认知
机(neocognitron)可以看作是卷积神经网络的第一个实现网
络，也是感受野概念在人工神经网络领域的首次应用。

CNN通过局部感受野和权值共享减少了神经网络需要训练的参数个数。

权值共享：训练大小只跟窗口大小有关。5*5 = 25

###### 卷积计算

卷积核/滤波器 ， 滑动扫描全部点 ， 得到特征图 feature map

步长越大，得到特征图越小

卷积核里面的值，是根据训练结果来进行重新计算

###### 池化

Pooling常用的三种方式

1. max pooling 最大池化

2. mean-pooling 平均池化

3. stochastic pooling  随机池化

###### 卷积Padding

SAME PADDING:

1. 给平面外部补0
2. 卷积窗口采样后得到一个跟原来大小相同的平面

VALID PADDING:

1. 不会超出平面外部
2. 卷积窗口采样后得到一个比原来平面小的平面

SAME PADDING:可能会给平面外部补0
        VALID PADDING:不会超出平面外部

假如有一个28\*28的平面，用2\*2步长为2的窗口对其进行pooling操作
                使用SAME PADDING的方式，得到14\*14的平面
               使用VALID PADDING的方式，得到14*14的平面

假如有一个2\*3的平面，用2\*2步长为2的窗口对其进行pooling操作
				使用SAME PADDING的方式，得到1\*2的平面
				使用VALID PADDING的方式，得到1*1的平面

##### 循环神经网络

RNN，上个世纪八十年代

##### 长短时记忆网络

LSTM