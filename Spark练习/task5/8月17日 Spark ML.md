# 8月17日分类和回归



### 逻辑回归

#### logistic分布

设X是连续随机变量，X服从logistic分布分布是指X具有下列分布函数和密度函数：

$F(x) = P(x<= x)=\frac{1}{1+e^{-(x-\mu)/{\gamma}}}$

$f(x)=F'(x)=\frac{e^{e-(x-\mu)/{\gamma}}}{\gamma(1+e^{-(x-\mu)/{\gamma}})^2}$

其中，$\mu$为位置参数，$\gamma$为形状参数，f(x)与F(x)图像如下，其中分布函数是以$(\mu,\frac{1}{2})$为中心对称，$\gamma$越小，曲线变化越快。

![](F:\Typora_repo\SparkMLlib\6d96cc41gw1etfkt9bbhwj20c603xmx4.jpg)

#### 二项logistic回归模型

二项logistic回归模型如下：

$P(Y = 1|x) = \frac{exp(\omega \cdot x +b)}{1+exp(\omega \cdot x +b)}$

$P(Y = 0|x) = \frac{1}{1+exp(\omega \cdot x +b)}$

其中 $x \in R^n$ 是输入，$Y \in 0,1$是输出，W成为权值向量，b称为偏置， $\omega \cdot x$是$\omega$ 和 $x$的 内积。

##### 参数估计

假设：

$P(Y=1|x)=\pi(x)，P(Y=0|x)=1-\pi(x)$

则似然函数为：

$\prod_{i=1}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i} $

求对数似然函数：

$L(\omega) =\sum_{i=1}^{N}[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]$

$\sum_{i=1}^{N}[y_ilog{\frac{\pi(x_i)}{1-\pi(x_i)}}+log(1-\pi(x_i))]$

从而对: $L(\omega )$

求极大值，的搭配$\omega$的估计值。求极值的方法可以是梯度下降法，梯度上升法等。

### 决策树

#### 方法简介

 决策树（decision tree）是一种基本的分类与回归方法，这里主要介绍用于分类的决策树。决策树模式呈树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。学习时利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。

#### 基本原理

##### 特征选择

特征选择在于选区对训练数据具有分类能力的特征，这样可以一高决策树的效率。通常特征选择的准则是信息增益（或信息增益比，基尼指数等），每次计算每个特征的信息增益，并比较他们的大小，选择西信息增益最大（信息增益比最大，基尼指数最小）的特征。

首先定义信息论中管饭使用的一个度量标准-熵(entropy)，他表示随机变量不确定性的度量/熵越大，随机变量的不确定越大。而信息增益（information entropy）表示得知某一特征后是的信息不确定性减少的程度。

信息增益：特征A对训练数据集D的信息增益

​	$g(D,A) = H(D) - H(D|A)$

$H(D)$为集合D的经验熵

$H(D|A)$特征A给定条件下D的经验条件熵

信息增益比：

$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$

$g(D,A)$为信息增益

$H_A(D)$与训练数据集D关于特征A的值的熵

$H_A(D) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$

n是特征A取值的个数

##### 尼基指数

基尼指数：分类问题中，假设有K个类，样本属于第K类的概率为$p_k$

则概率分布的基尼指数定义为

​	$Gini(p) = \sum_{k=1}^{K}(1-p_k) = 1 - \sum_{k=1}^{k}{p_k^2}$

#### 决策树的生成

 从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增均很小或没有特征可以选择为止，最后得到一个决策树。

 决策树需要有停止条件来终止其生长的过程。一般来说最低的条件是：当该节点下面的所有记录都属于同一类，或者当所有的记录属性都具有相同的值时。这两种条件是停止决策树的必要条件，也是最低的条件。在实际运用中一般希望决策树提前停止生长，限定叶节点包含的最低数据量，以防止由于过度生长造成的过拟合问题。

#### 决策树的剪枝

决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化，这个过程称为剪枝。

 决策树的剪枝往往通过极小化决策树整体的损失函数来实现。一般来说，损失函数可以进行如下的定义：

  $C_a(T)=C(T)+a\left|T\right|$

 其中，T为任意子树，

  $C(T)$

为对训练数据的预测误差（如基尼指数），

$|T|$

为子树的叶结点个数，

$ a\ge0$

为参数，

$C_a(T)$

为参数是

  $a$

时的子树T的整体损失，参数

$  a$

权衡训练数据的拟合程度与模型的复杂度。对于固定的

$  a$

，一定存在使损失函数

$  C_a(T)$

最小的子树，将其表示为

$  T_a$

。当

  $a$

大的时候，最优子树

  T_a

偏小；当

  $a$

小的时候，最优子树

  $T_a$

偏大。