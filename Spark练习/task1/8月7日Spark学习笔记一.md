8月7日

# Spark学习笔记 Task1

### 生态系统

Spark的生态系统主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件，各个组件的具体功能如下：

-  Spark Core：Spark Core包含Spark的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。Spark建立在统一的抽象RDD之上，使其可以以基本一致的方式应对不同的大数据处理场景；通常所说的Apache Spark，就是指Spark Core；
- Spark SQL：Spark SQL允许开发人员直接处理RDD，同时也可查询Hive、HBase等外部数据源。Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行查询，并进行更复杂的数据分析；
-  Spark Streaming：Spark Streaming支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流式计算分解成一系列短小的批处理作业。Spark Streaming支持多种数据输入源，如Kafka、Flume和TCP套接字等；
- MLlib（机器学习）：MLlib提供了常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等，降低了机器学习的门槛，开发人员只要具备一定的理论知识就能进行机器学习的工作；
- GraphX（图计算）：GraphX是Spark中用于图计算的API，可认为是Pregel在Spark上的重写及优化，Graphx性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。

### 运行架构

##### 基本概念

- RDD：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型；
- DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系;
- Executor：是运行在工作节点（Worker Node）上的一个进程，负责运行任务，并为应用程序存储数据；
- 应用：用户编写的Spark应用程序；
- 任务：运行在Executor上的工作单元；
- 作业：一个作业包含多个RDD及作用于相应RDD上的各种操作；
- 阶段：是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”，或者也被称为“任务集”。

##### 架构设计

spark运行架构包括资源管理器（Cluster Manager），运行作业任务的工作节点(Worker Node)、每个应用的任务控制节点（Driver）和每个应用的任务控制节点上负责具体任务的执行进程（Executor）。其中，集群资源管理器可以是Spark自带的资源管理器，也可以是YARN或Mesos等资源管理框架。

Spark所采用的Executor有两个优点：一是利用多线程来执行具体的任务，减少人物的启动开销；而是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这存储模块里，下次需要时，就可以直接读该存储模块里的数据，因而有效减少了IO开销。

![](E:\Typora_Config\spark\图9-5-Spark运行架构.jpg)

在Spark中，一个应用（Application）由有个任务控制节点（Driver）和若干个作业（Job）构成，一个作业由多个阶段（Stage）构成，一个阶段有多个任务（Task）组成。当一个应用执行时，任务控制节点会向集群管理器（Cluster Manager）申请资源，启动Executor，并向Executor上执行任务，运行结束后，执行结构会返回给任务控制节点，或者写到HDFS或者其他数据库。

![图9-6-Spark中各种概念之间的相互关系](E:\Typora_Config\spark\图9-6-Spark中各种概念之间的相互关系.jpg)

##### Spark基本运行流程

1.当一个Spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即有任务控制节点（Driver）创建一个SparkContext,由SparkContext负责和资源管理器（Cluster Manager）的通信以及进行资源的申请，任务的分配和监控等。SparkContext会向资源管理器注册申请运行Executor的资源。

2.资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上。

3.SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器（DAGScheduler）进行解析，将DAG图分解成多个“阶段”（每个阶段都是一个任务集），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器（TaskScheduler）进行处理；Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行，同时，SparkContext将应用程序代码发放给Executor；

4.任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。

![](E:\Typora_Config\spark\图9-7-Spark运行基本流程图.jpg)

spark运行架构具有以下特点：

1.每个应用都有自己专属的Eecutor进程，并且该进程在应用运行期间一直驻留。Executor进程以多线程的方式运行任务，减少了多进程任务频繁的启动开销，使得任务执行变得非常高效和可靠。

2.spark运行过程于资源管理器无关，只要能够获取executor进程并保持同意才能即可。

3.Executor上有一个BlockManager存储模块，类似于键值存储系统（把内存和磁盘共同作为存储设备），在处理迭代计算任务时，不需要把中间结果写入到HDFS等文件系统，而是直接放在这个存储系统上，后续有需要时就可以直接读取；在交互式查询场景下，也可以把表提前缓存到这个存储系统上，提高读写IO性能；

4.任务采用了数据本地性和推测执行等优化机制。数据本地性是尽量将计算移到数据所在的节点上进行，即“计算向数据靠拢”，因为移动计算比移动数据所占的网络资源要少得多。而且，Spark采用了延时调度机制，可以在更大的程度上实现执行过程优化。比如，拥有数据的节点当前正被其他的任务占用，那么，在这种情况下是否需要将数据移动到其他的空闲节点呢？答案是不一定。因为，如果经过预测发现当前节点结束当前任务的时间要比移动数据的时间还要少，那么，调度就会等待，直到当前节点可用。

### RDD设计

迭代式算法（机器学习，图算法）和交互式数据挖掘工具，这些共同之处有，不同计算阶段会冲涌中间结果，即一个阶段的输出结果会作为下一个阶段的输入。目前，MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制，磁盘IO和序列化开销。Pregel等图计算框架也是将结果保存在内存中，但是这些框架只能支持一些特定的计算模式，并没有提供一种通用的数据抽象。

##### 基本概念

一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分为多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中的不同的节点上，从而可以在集群中的不同节点上进行并行计算。

RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区集合，不能字节修改，只能给予稳定的物理存储中的数据来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map,join和groupBy）而创建的到新的RDD。

RDD提供一组封堵的操作以支持常见的数据运算，分为“行动”(Action)和“转换”(Transformation)两种类型，前者用于执行计算并制定输出的形式，或者制定RDD之间的相互依赖关系。

转换操作（比如map,filter,groupBy,join等）接受RDD并返回RDD，而行动操作（比如count,collect等）接受RDD但是返回一个值或者结果。RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改。因此，RDD比较适合对于数据集中元素执行相同操作的批处理式应用，而不适合用于需要异步、细粒度状态的应用，比如Web应用系统、增量式的网页爬虫等。正因为这样，这种粗粒度转换接口设计，会使人直觉上认为RDD的功能很受限、不够强大。但是，实际上RDD已经被实践证明可以很好地应用于许多并行计算应用中，可以具备很多现有计算框架（比如MapReduce、SQL、Pregel等）的表达能力，并且可以应用于这些框架处理不了的交互式数据挖掘应用。

###### RDD典型执行过程

1. RDD读入外部数据源（或内存中的集合）进行创建；
2. RDD经过一系列的“转换”操作，每一次都会产生不同的RDD，给下一个“转换”使用
3. 最后一个RDD经“行动”操作进行处理，并输出到外部数据源（或者编程集合或标量）

RDD才用了懒调用，即在RDD的执行过程中，真正的计算发生在RDD的“行动”操作，对于“行动”之前的所有“转换”操作，spark只是进行记录下“转换”操作应用的一些基础数据集以及RDD生成的轨迹，即相互之间的依赖关系，而不会触发真正的计算。

![](E:\Typora_Config\spark\RDD\图9-8-Spark的转换和行动操作.jpg)

在下图中，逻辑上生成A和C两个RDD，经过一系列“转换”操作，逻辑上生成了F（也是一个RDD），之所以说是逻辑上，是因为这时候计算并没有发生，Spark知识记录了RDD之间的生成和依赖关系。当F要进行输出时，也就是当F进行“行动”操作的时候，Spark会根据RDD的依赖关系生成DAG，并从起点开始真正的计算。

![](E:\Typora_Config\spark\RDD\图9-9-RDD执行过程的一个实例.jpg)

上述致一系列处理成为一个“血缘关系（Lineage）”,即DAG拓扑排序的结果。

```py
fileRDD = sc.textFile('./dataset/hello.txt')
def contains(line):
    return 'hello world!' in line
filterRDD = fileRDD.filter(contains)
filterRDD.cache()
filterRDD.count()
```

第1行代码从HDFS文件中读取数据创建一个RDD；第2、3行定义一个过滤函数;第4行代码对fileRDD进行转换操作得到一个新的RDD，即filterRDD；第5行代码表示对filterRDD进行持久化，把它保存在内存或磁盘中（这里采用cache接口把数据集保存在内存中），方便后续重复使用，当数据被反复访问时（比如查询一些热点数据，或者运行迭代算法），这是非常有用的，而且通过cache()可以缓存非常大的数据集，支持跨越几十甚至上百个节点；第5行代码中的count()是一个行动操作，用于计算一个RDD集合中包含的元素个数。

##### RDD特性

1.高效的容错性。有的分布式共享内存、键值存储、内存数据库等，为了实现容错，必须在集群节点之间进行数据复制或者记录日志，也就是在节点之间会发生大量的数据传输，这对于数据密集型应用而言会带来很大的开销。在RDD的设计中，数据只读，不可修改，如果需要修改数据，必须从父RDD转换到子RDD，由此在不同RDD之间建立了血缘关系。所以，RDD是一种天生具有容错机制的特殊集合，不需要通过数据冗余的方式（比如检查点）实现容错，而只需通过RDD父子依赖（血缘）关系重新计算得到丢失的分区来实现容错，无需回滚整个系统，这样就避免了数据复制的高开销，而且重算过程可以在不同节点之间并行进行，实现了高效的容错。此外，RDD提供的转换操作都是一些粗粒度的操作（比如map、filter和join），

2.中间结果持久化到内存

3.存放的数据可以是java对象

##### RDD之间的依赖关系

RDD中不同的操作会使得不同RDD种的分区会产生不同的依赖。RDD中的依赖关系分为宽依赖（Wide Dependency）和窄依赖（Narrow Dependency）.

![](E:\Typora_Config\spark\RDD\图9-10-窄依赖与宽依赖的区别.jpg)

窄依赖表现为一个父RDD对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。（map,filter,union等）

宽依赖表现为存在一个父RDD的一个分区对应一个子RDD的多个分区。(groupByKey，sortByKey)等。

对于join可以分两种：

对输入进行协同划分，属于窄依赖。所谓协同划分（co-partitioned）是指多个父RDD的某一分区的所有“键（key）”，落在子RDD的同一个分区内，不会产生同一个父RDD的某一分区，落在子RDD的两个分区的情况。

对输入做非协同划分，属于宽依赖。对于窄依赖的RDD，可以以流水线的方式计算所有父分区，不会造成网络之间的数据混合。对于宽依赖的RDD，则通常伴随着Shuffle操作，即首先需要计算好所有父分区数据，然后在节点之间进行Shuffle。

##### 阶段的划分

Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分阶段，具体划分方法是：在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到当前的阶段中；将窄依赖尽量划分在同一个阶段中，可以实现流水线计算（具体的阶段划分算法请参见AMP实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》）。例如，如图9-11所示，假设从HDFS中读入数据生成3个不同的RDD（即A、C和E），通过一系列转换操作后再将计算结果保存回HDFS。对DAG进行解析时，在依赖图中进行反向解析，由于从RDD A到RDD B的转换以及从RDD B和F到RDD G的转换，都属于宽依赖，因此，在宽依赖处断开后可以得到三个阶段，即阶段1、阶段2和阶段3。可以看出，在阶段2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作，比如，分区7通过map操作生成的分区9，可以不用等待分区8到分区9这个转换操作的计算结束，而是继续进行union操作，转换得到分区13，这样流水线执行大大提高了计算的效率。

![](E:\Typora_Config\spark\RDD\图9-11-根据RDD分区的依赖关系划分阶段.jpg)

由上述论述可知，把一个DAG图划分成多个“阶段”以后，每个阶段都代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集合。每个任务集合会被提交给任务调度器（TaskScheduler）进行处理，由任务调度器将任务分发给Executor运行。

##### RDD运行过程

通过上述对RDD概念、依赖关系和阶段划分的介绍，结合之前介绍的Spark运行基本流程，这里再总结一下RDD在Spark架构中的运行过程（如图9-12所示）：
（1）创建RDD对象；
（2）SparkContext负责计算RDD之间的依赖关系，构建DAG；
（3）DAGScheduler负责把DAG图分解成多个阶段，每个阶段中包含了多个任务，每个任务会被任务调度器分发给各个工作节点（Worker Node）上的Executor去执行。

![](E:\Typora_Config\spark\RDD\图9-12-RDD在Spark中的运行过程.jpg)